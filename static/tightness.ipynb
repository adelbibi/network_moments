{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Network Moments (GNMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Moments (NMs).\n",
      "\n",
      "Let x be a random variable with some mean M and covariance S.\n",
      "x can be multivariate of size (N), so S of size (N, N) and M of size (N).\n",
      "S = C - outer_product(M, M), where C is the correlation matrix.\n",
      "M and C are the expectations of x and outer_product(x, x), respectively.\n",
      "The n-th moment of x is the expectation of x_to_the_power_n.\n",
      "The diagonal of S and C are the variance and second_moment, respectively.\n",
      "The second_moment is the expectation of x_squared.\n",
      "The variance = second_moment - M_squared.\n",
      "\n",
      "For any function acting on x (e.g., f(x)),\n",
      "we want to compute its probability density function (i.e., of f(x)).\n",
      "A simpler task maybe is to find the n-th-moment of the function for all n > 0.\n",
      "\n",
      "This module is trying to find closed form expressions for the output\n",
      "probabilistic moments of some functions given some input distributions.\n",
      "\n",
      "Network Moments Structure:\n",
      "[{'gaussian': [{'affine': ['covariance', 'mean', 'variance']},\n",
      "               {'affine_relu_affine': ['mean',\n",
      "                                       'special_covariance',\n",
      "                                       'special_variance']},\n",
      "               {'relu': ['mean',\n",
      "                         'zero_mean_correlation',\n",
      "                         'zero_mean_covariance']}]},\n",
      " {'utils': ['diagonal',\n",
      "            'epsilon',\n",
      "            'flatten',\n",
      "            'jacobian',\n",
      "            'linearize',\n",
      "            'mul_diag',\n",
      "            'normalize',\n",
      "            'normalize_',\n",
      "            'outer',\n",
      "            {'rand': ['definite', 'from_eigen']}]}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pprint import pprint\n",
    "from types import ModuleType\n",
    "import network_moments.torch as nm\n",
    "\n",
    "seed = 73  # for reproducability\n",
    "\n",
    "def traverse(obj, exclude=[]):\n",
    "    data = []\n",
    "    if type(obj) is not ModuleType:\n",
    "        return data\n",
    "    for e in dir(obj):\n",
    "        if not e.startswith('_') and all(e != s for s in exclude):\n",
    "            sub = traverse(obj.__dict__[e], exclude)\n",
    "            data.append(e if len(sub) == 0 else {e:sub})\n",
    "    return data\n",
    "\n",
    "print(nm.__doc__)\n",
    "print('Network Moments Structure:')\n",
    "pprint(traverse(nm, exclude=['tests', 'general']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the tightness of the expressions on ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tightness in module network_moments.gaussian.relu.tests:\n",
      "\n",
      "tightness(length=1, count=1000000, seed=None, dtype=torch.float64, device=None)\n",
      "    Test the tightness of the expressions against Monte-Carlo estimations.\n",
      "    \n",
      "    The expressions are for the ReLU function f(x) = max(x, 0).\n",
      "    \n",
      "    Args:\n",
      "        length: Size of the vector.\n",
      "        count: Number of samples for Monte-Carlo estimation.\n",
      "        seed: Seed for the random number generator.\n",
      "        dtype: The data type.\n",
      "        device: In which device.\n",
      "    \n",
      "    Returns:\n",
      "        (out_mu, mc_mu), (out_var, mc_var)\n",
      "        out_mu: The output mean computed using the expressions.\n",
      "        mc_mu: The output mean estimated using Monte-Carlo sampling.\n",
      "        out_var: The output variance computed using the expressions.\n",
      "        mc_var: The output variance estimated using Monte-Carlo sampling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nm.gaussian.relu.tests.tightness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte-Carlo mean / Analytical mean:\n",
      "[0.99934677]\n",
      "Monte-Carlo variance / Analytical variance:\n",
      "[0.99964285]\n"
     ]
    }
   ],
   "source": [
    "(out_mu, mc_mu), (out_var, mc_var) = nm.gaussian.relu.tests.tightness(1, seed=seed)\n",
    "print('Monte-Carlo mean / Analytical mean:')\n",
    "print((mc_mu / out_mu).cpu().numpy())\n",
    "print('Monte-Carlo variance / Analytical variance:')\n",
    "print((mc_var / out_var).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the tightness of the expressions on the affine transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte-Carlo mean / Analytical mean:\n",
      "[0.99832705 0.99963103 0.99941707]\n",
      "Monte-Carlo variance / Analytical variance:\n",
      "[0.99934219 0.99814858 1.00044002]\n"
     ]
    }
   ],
   "source": [
    "(out_mu, mc_mu), (out_var, mc_var) = nm.gaussian.affine.tests.tightness(3, seed=seed)\n",
    "print('Monte-Carlo mean / Analytical mean:')\n",
    "print((mc_mu / out_mu).cpu().numpy())\n",
    "print('Monte-Carlo variance / Analytical variance:')\n",
    "print((mc_var / out_var).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the correctness of the batch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the correctness of batch implementation of mean().\n",
      "\n",
      "    This function will stack `[1 * mu, 2 * mu, ..., batch * mu]`.\n",
      "    Then, it will see whether the batch output is accurate or not.\n",
      "\n",
      "    Args:\n",
      "        size: Tuple size of matrix A.\n",
      "        batch: The batch size > 0.\n",
      "        dtype: data type.\n",
      "        device: In which device.\n",
      "        mu: To test a specific mean mu.\n",
      "        A: To test a specific A matrix.\n",
      "        b: To test a specific bias b.\n",
      "\n",
      "    Returns:\n",
      "        A scalar, the closer it is to zero,\n",
      "        the more accurate the implementation.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(nm.general.affine.tests.batch_mean.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If these values are close to zero, the batch implementation is correct:\n",
      "2.220446049250313e-16\n",
      "(0.0, 0.0)\n",
      "(2.220446049250313e-16, 5.551115123125783e-17)\n",
      "(0.0, 0.0, 0.0)\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print('If these values are close to zero, the batch implementation is correct:')\n",
    "print(nm.general.affine.tests.batch_mean())\n",
    "print(nm.general.affine.tests.batch_variance())\n",
    "print(nm.general.affine.tests.batch_covariance())\n",
    "print(nm.gaussian.relu.tests.batch_mean())\n",
    "print(nm.gaussian.relu.tests.batch_zero_mean_correlation())\n",
    "print(nm.gaussian.relu.tests.batch_zero_mean_covariance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the tightness of the expressions on affine-ReLU-affine networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output variance of Affine-ReLU-Affine for special Gaussian input.\n",
      "\n",
      "    f(x) = B*max(A*x+c1, 0)+c2, where c1 = -A*input_mean.\n",
      "\n",
      "    For this specific c1, this function doesn't depend on\n",
      "    neither the input mean nor the biases.\n",
      "\n",
      "    Args:\n",
      "        covariance: Input covariance matrix (Batch, Size, Size)\n",
      "            or variance vector (Batch, Size) for diagonal covariance.\n",
      "        A: The A matrix (M, Size).\n",
      "        B: The B matrix (N, M).\n",
      "        variance: Whether the input covariance is a diagonal matrix.\n",
      "        stability: For accurate results this should be zero\n",
      "            if used in training, use a value like 1e-4 for stability.\n",
      "\n",
      "    Returns:\n",
      "        Output variance of Affine-ReLU-Affine for Gaussian input\n",
      "        with mean = `mean` and covariance matrix = `covariance`\n",
      "        where the bias of the first affine = -A*`mean`.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "rand = nm.utils.rand\n",
    "gnm = nm.gaussian.affine_relu_affine\n",
    "print(gnm.special_variance.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte-Carlo mean / Analytical mean:\n",
      "[0.99859776 0.9986586  1.00122188]\n",
      "Monte-Carlo variance / Analytical variance:\n",
      "[1.00007968 0.99763406 0.99830018]\n"
     ]
    }
   ],
   "source": [
    "length = 3\n",
    "count = 1000000\n",
    "dtype = torch.float64\n",
    "device = torch.device('cpu', 0)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# input mean and covariance\n",
    "mu = torch.randn(length, dtype=dtype, device=device)\n",
    "cov = rand.definite(length, dtype=dtype, device=device,\n",
    "                    positive=True, semi=False, norm=1.0)\n",
    "\n",
    "# variables\n",
    "A = torch.randn(length, length, dtype=dtype, device=device)\n",
    "c1 = -A.matmul(mu)  # torch.randn(length, dtype=dtype)\n",
    "B = torch.randn(length, length, dtype=dtype, device=device)\n",
    "c2 = torch.randn(length, dtype=dtype, device=device)\n",
    "\n",
    "# analytical output mean and variance\n",
    "out_mu = gnm.mean(mu, cov, A, c1, B, c2)\n",
    "out_var = gnm.special_variance(cov, A, B)\n",
    "\n",
    "# Monte-Carlo estimation of the output mean and variance\n",
    "normal = torch.distributions.MultivariateNormal(mu, cov)\n",
    "samples = normal.sample((count,))\n",
    "out_samples = samples.matmul(A.t()) + c1\n",
    "out_samples = torch.max(out_samples, torch.zeros([], dtype=dtype, device=device))\n",
    "out_samples = out_samples.matmul(B.t()) + c2\n",
    "mc_mu = torch.mean(out_samples, dim=0)\n",
    "mc_var = torch.var(out_samples, dim=0)\n",
    "\n",
    "# printing the ratios\n",
    "print('Monte-Carlo mean / Analytical mean:')\n",
    "print((mc_mu / out_mu).cpu().numpy())\n",
    "print('Monte-Carlo variance / Analytical variance:')\n",
    "print((mc_var / out_var).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tightness of A (best is zero): 0.0\n",
      "Tightness of b (best is zero): 0.0\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "num_classes = 10\n",
    "image_size = (28, 28)\n",
    "dtype = torch.float64\n",
    "device = torch.device('cpu', 0)\n",
    "\n",
    "size = torch.prod(torch.tensor(image_size)).item()\n",
    "x = torch.rand(batch, *image_size, dtype=dtype, device=device)\n",
    "model = nn.Sequential(\n",
    "    nm.utils.flatten,\n",
    "    nn.Linear(size, num_classes),\n",
    ")\n",
    "model.type(dtype)\n",
    "if device.type != 'cpu':\n",
    "    model.cuda(device.index)\n",
    "\n",
    "jac, bias = nm.utils.linearize(model, x)\n",
    "A = list(model.children())[1].weight\n",
    "print('Tightness of A (best is zero): {}'.format(\n",
    "    torch.max(torch.abs(jac - A)).item()))\n",
    "\n",
    "b = list(model.children())[1].bias\n",
    "print('Tightness of b (best is zero): {}'.format(\n",
    "    torch.max(torch.abs(bias - b)).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-stage linearization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte-Carlo mean / Analytical mean:\n",
      "[0.99706612 0.99764148 0.99880015 1.00019028 1.00096126 0.99965541\n",
      " 0.99916396 0.99982956 0.99944559 0.99710325]\n",
      "Monte-Carlo variance / Analytical variance:\n",
      "[1.28257357 1.52839872 1.34012299 0.98683863 1.08856951 1.27928087\n",
      " 1.26852927 1.32427244 1.1481256  1.2438608 ]\n"
     ]
    }
   ],
   "source": [
    "count = 10000\n",
    "num_classes = 10\n",
    "image_size = (28, 28)\n",
    "dtype = torch.float64\n",
    "device = torch.device('cpu', 0)\n",
    "gnm = nm.gaussian.affine_relu_affine\n",
    "\n",
    "size = torch.prod(torch.tensor(image_size)).item()\n",
    "x = torch.rand(1, *image_size, dtype=dtype, device=device)\n",
    "\n",
    "# deep model\n",
    "first_part = nn.Sequential(\n",
    "    nm.utils.flatten,\n",
    "    nn.Linear(size, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 300),\n",
    ")\n",
    "first_part.type(dtype)\n",
    "relu = nn.Sequential(\n",
    "    nn.ReLU(),\n",
    ")\n",
    "relu.type(dtype)\n",
    "second_part = nn.Sequential(\n",
    "    nn.Linear(300, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, num_classes),\n",
    ")\n",
    "second_part.type(dtype)\n",
    "if device.type != 'cpu':\n",
    "    first_part.cuda(device.index)\n",
    "    relu.cuda(device.index)\n",
    "    second_part.cuda(device.index)\n",
    "def model(x):\n",
    "    return second_part(relu(first_part(x)))\n",
    "\n",
    "# variables\n",
    "A, c1 = nm.utils.linearize(first_part, x)\n",
    "B, c2 = nm.utils.linearize(second_part, relu(first_part(x)).detach())\n",
    "x.requires_grad_(False)\n",
    "A.squeeze_()\n",
    "c1.squeeze_()\n",
    "B.squeeze_()\n",
    "c2.squeeze_()\n",
    "\n",
    "# analytical output mean and variance\n",
    "mean = x.view(-1)\n",
    "covariance = rand.definite(size, norm=0.1, dtype=dtype, device=device)\n",
    "out_mu = gnm.mean(mean, covariance, A, c1, B, c2)\n",
    "out_var = gnm.special_variance(covariance, A, B)\n",
    "\n",
    "# Monte-Carlo estimation of the output mean and variance\n",
    "normal = torch.distributions.MultivariateNormal(mean, covariance)\n",
    "samples = normal.sample((count,))\n",
    "out_samples = model(samples.view(-1, *image_size)).detach()\n",
    "mc_mu = torch.mean(out_samples, dim=0)\n",
    "mc_var = torch.var(out_samples, dim=0)\n",
    "\n",
    "# printing the ratios\n",
    "print('Monte-Carlo mean / Analytical mean:')\n",
    "print((mc_mu / out_mu).cpu().numpy())\n",
    "print('Monte-Carlo variance / Analytical variance:')\n",
    "print((mc_var / out_var).cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
